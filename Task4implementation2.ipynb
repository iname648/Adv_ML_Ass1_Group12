{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T12:22:50.035993Z","iopub.status.busy":"2024-09-30T12:22:50.035326Z","iopub.status.idle":"2024-09-30T12:32:26.568800Z","shell.execute_reply":"2024-09-30T12:32:26.567755Z","shell.execute_reply.started":"2024-09-30T12:22:50.035950Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n","100%|██████████| 74.5M/74.5M [00:00<00:00, 199MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:04<00:00, 34851200.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1, Loss: 1.665076119275865\n","Epoch 2, Loss: 1.0600827824055044\n","Epoch 3, Loss: 0.8666328900873241\n","Test Accuracy on CIFAR-10: 79.99%\n"]}],"source":["import torch\n","from torchvision import models\n","\n","model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","model.eval()\n","\n","weights = models.EfficientNet_B4_Weights.DEFAULT.get_state_dict(progress=True)\n","\n","import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","efficientnet_b4_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","efficientnet_b4_model.classifier[1] = nn.Linear(efficientnet_b4_model.classifier[1].in_features, 10) \n","\n","for param in efficientnet_b4_model.features.parameters():\n","    param.requires_grad = False\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(efficientnet_b4_model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","efficientnet_b4_model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    efficientnet_b4_model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = efficientnet_b4_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","efficientnet_b4_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = efficientnet_b4_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T12:32:26.571926Z","iopub.status.busy":"2024-09-30T12:32:26.571236Z","iopub.status.idle":"2024-09-30T13:36:00.426467Z","shell.execute_reply":"2024-09-30T13:36:00.425479Z","shell.execute_reply.started":"2024-09-30T12:32:26.571873Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n","100%|██████████| 331M/331M [00:06<00:00, 51.6MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1, Loss: 0.36059003191155764\n","Epoch 2, Loss: 0.1526837245679162\n","Epoch 3, Loss: 0.13233575544791368\n","Test Accuracy on CIFAR-10: 95.27%\n"]}],"source":["import torch\n","from torchvision import models\n","\n","vit_b16_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","vit_b16_model.eval()\n","\n","weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.get_state_dict(progress=True)\n","\n","import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","vit_b16_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","vit_b16_model.heads.head = nn.Linear(vit_b16_model.heads.head.in_features, 10)  \n","\n","for param in vit_b16_model.parameters():\n","    param.requires_grad = False\n","\n","for param in vit_b16_model.heads.parameters():\n","    param.requires_grad = True\n","\n","transform = transforms.Compose([\n","    transforms.Resize((384, 384)), \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(vit_b16_model.heads.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vit_b16_model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    vit_b16_model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = vit_b16_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","vit_b16_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = vit_b16_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-30T13:36:00.428597Z","iopub.status.busy":"2024-09-30T13:36:00.428168Z","iopub.status.idle":"2024-09-30T13:45:12.858074Z","shell.execute_reply":"2024-09-30T13:45:12.857077Z","shell.execute_reply.started":"2024-09-30T13:36:00.428551Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 48456864.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n","Files already downloaded and verified\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c56ccb613384068af56dbe118acd234","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/240 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fc4bca0f62947d6b6830a32056fd7a5","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0/240 [00:00<?, ?files/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b245607b5a8466bbcbb0b3ab2dcb847","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving shape bias dataset with edge detection method...\n","Saving texture bias dataset with DTD textures...\n","Saving color bias dataset...\n","Datasets saved successfully!\n","ViT_B_16 Shape Bias Accuracy: 9.53%\n","ViT_B_16 Texture Bias Accuracy: 10.80%\n","ViT_B_16 Color Bias Accuracy: 16.00%\n","EfficientNet_B4 Shape Bias Accuracy: 10.13%\n","EfficientNet_B4 Texture Bias Accuracy: 9.00%\n","EfficientNet_B4 Color Bias Accuracy: 9.67%\n"]}],"source":["import os\n","import torch\n","from torchvision import datasets, transforms\n","import torchvision.transforms.functional as F\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import random\n","from datasets import load_dataset\n","\n","shape_dir = './data/shape'\n","texture_dir = './data/texture'\n","color_dir = './data/color'\n","\n","os.makedirs(shape_dir, exist_ok=True)\n","os.makedirs(texture_dir, exist_ok=True)\n","os.makedirs(color_dir, exist_ok=True)\n","\n","transform = transforms.Compose([\n","    transforms.Resize((384, 384)),\n","    transforms.Grayscale(num_output_channels=3),  \n","    transforms.ToTensor(),\n","])\n","cifar10_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n","cifar10_test = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n","\n","cifar10_data = cifar10_train + cifar10_test\n","\n","def edge_transform(img):\n","    img = np.array(img) \n","    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n","    edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)  \n","    \n","    edges_rgb = np.stack((edges,)*3, axis=-1) \n","    return Image.fromarray(edges_rgb) \n","\n","data_augmentation = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10), \n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  \n","])\n","\n","shape_transform = transforms.Compose([\n","    data_augmentation, \n","    transforms.Lambda(lambda img: edge_transform(img)), \n","    transforms.Resize((384, 384)),\n","    transforms.ToTensor()\n","])\n","texture_dataset = load_dataset(\"/kaggle/input/texture\", split=\"train\")\n","\n","def get_texture_images(dataset):\n","    textures = []\n","    for item in dataset:\n","        img = item['image'] if isinstance(item['image'], Image.Image) else Image.open(item['image']).convert('RGB')\n","        textures.append(transforms.Resize((384, 384))(img))  \n","    return textures\n","\n","texture_images = get_texture_images(texture_dataset)\n","\n","def blend_with_texture(img, texture):\n","    alpha = 0.5  \n","    img_tensor = transforms.ToTensor()(img) if not isinstance(img, torch.Tensor) else img\n","    texture_tensor = transforms.ToTensor()(texture) if not isinstance(texture, torch.Tensor) else texture\n","    \n","    if img_tensor.size() != texture_tensor.size():\n","        texture_tensor = F.resize(texture_tensor, img_tensor.size()[1:]) \n","\n","    blended_tensor = (alpha * img_tensor) + ((1 - alpha) * texture_tensor)\n","    \n","    blended_image = transforms.ToPILImage()(blended_tensor.clamp(0, 1)) \n","    return blended_image.convert('RGB')  \n","\n","def apply_texture_transform(img):\n","    texture_img = random.choice(texture_images) \n","    blended_image = blend_with_texture(img, texture_img)  \n","    return blended_image  \n","\n","texture_transform = transforms.Compose([\n","    data_augmentation,  \n","    transforms.Lambda(lambda x: apply_texture_transform(x)),  \n","    transforms.Resize((384, 384)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","color_transform = transforms.Compose([\n","    data_augmentation, \n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((384, 384)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","def save_images_limited(dataset, transform, save_dir, limit=1500):\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","    \n","    for i, (img, label) in enumerate(dataset):\n","        if i >= limit:\n","            break\n","        \n","        img_pil = transforms.ToPILImage()(img) \n","        img_transformed = transform(img_pil.convert('RGB')) \n","        img_pil_transformed = transforms.ToPILImage()(img_transformed) \n","        \n","        class_dir = os.path.join(save_dir, str(label))\n","        if not os.path.exists(class_dir):\n","            os.makedirs(class_dir)\n","        \n","        img_pil_transformed.save(os.path.join(class_dir, f'image_{i}.png'))\n","\n","\n","print(\"Saving shape bias dataset with edge detection method...\")\n","save_images_limited(cifar10_data, shape_transform, shape_dir)\n","\n","print(\"Saving texture bias dataset with DTD textures...\")\n","save_images_limited(cifar10_data, texture_transform, texture_dir)\n","\n","print(\"Saving color bias dataset...\")\n","save_images_limited(cifar10_data, color_transform, color_dir)\n","\n","print(\"Datasets saved successfully!\")\n","\n","shape_dataset = datasets.ImageFolder(root='./data/shape', transform=shape_transform)\n","texture_dataset = datasets.ImageFolder(root='./data/texture', transform=texture_transform)\n","color_dataset = datasets.ImageFolder(root='./data/color', transform=color_transform)\n","\n","batch_size = 32\n","shape_loader = DataLoader(shape_dataset, batch_size=batch_size, shuffle=True)\n","texture_loader = DataLoader(texture_dataset, batch_size=batch_size, shuffle=True)\n","color_loader = DataLoader(color_dataset, batch_size=batch_size, shuffle=True)\n","\n","vit_b16_model = vit_b16_model.to(device)\n","efficientnet_b4_model = efficientnet_b4_model.to(device)\n","\n","def evaluate_model(model, data_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)  \n","            outputs = model(images)  \n","            _, predicted = torch.max(outputs, 1) \n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","shape_bias_acc_vit = evaluate_model(vit_b16_model, shape_loader, device)\n","texture_bias_acc_vit = evaluate_model(vit_b16_model, texture_loader, device)\n","color_bias_acc_vit = evaluate_model(vit_b16_model, color_loader, device)\n","\n","shape_bias_acc_effnet = evaluate_model(efficientnet_b4_model, shape_loader, device)\n","texture_bias_acc_effnet = evaluate_model(efficientnet_b4_model, texture_loader, device)\n","color_bias_acc_effnet = evaluate_model(efficientnet_b4_model, color_loader, device)\n","\n","print(f'ViT_B_16 Shape Bias Accuracy: {shape_bias_acc_vit:.2f}%')\n","print(f'ViT_B_16 Texture Bias Accuracy: {texture_bias_acc_vit:.2f}%')\n","print(f'ViT_B_16 Color Bias Accuracy: {color_bias_acc_vit:.2f}%')\n","\n","print(f'EfficientNet_B4 Shape Bias Accuracy: {shape_bias_acc_effnet:.2f}%')\n","print(f'EfficientNet_B4 Texture Bias Accuracy: {texture_bias_acc_effnet:.2f}%')\n","print(f'EfficientNet_B4 Color Bias Accuracy: {color_bias_acc_effnet:.2f}%')\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5790082,"sourceId":9511946,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
