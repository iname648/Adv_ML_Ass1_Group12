{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T11:52:48.220135Z","iopub.status.busy":"2024-09-24T11:52:48.219848Z","iopub.status.idle":"2024-09-24T11:52:54.097018Z","shell.execute_reply":"2024-09-24T11:52:54.096000Z","shell.execute_reply.started":"2024-09-24T11:52:48.220096Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n","100%|██████████| 74.5M/74.5M [00:00<00:00, 146MB/s] \n"]}],"source":["import torch\n","from torchvision import models\n","\n","model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","model.eval()\n","\n","weights = models.EfficientNet_B4_Weights.DEFAULT.get_state_dict(progress=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T11:52:54.099750Z","iopub.status.busy":"2024-09-24T11:52:54.099009Z","iopub.status.idle":"2024-09-24T12:05:43.880752Z","shell.execute_reply":"2024-09-24T12:05:43.879811Z","shell.execute_reply.started":"2024-09-24T11:52:54.099699Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 78097727.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1, Loss: 1.665056896987666\n","Epoch 2, Loss: 1.060906616411984\n","Epoch 3, Loss: 0.8677804567115245\n","Test Accuracy on CIFAR-10: 80.07%\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10) \n","\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T12:26:48.578445Z","iopub.status.busy":"2024-09-24T12:26:48.578080Z","iopub.status.idle":"2024-09-24T12:26:49.893683Z","shell.execute_reply":"2024-09-24T12:26:49.892693Z","shell.execute_reply.started":"2024-09-24T12:26:48.578410Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models\n","\n","model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","model.eval()\n","\n","weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.get_state_dict(progress=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T12:28:05.063439Z","iopub.status.busy":"2024-09-24T12:28:05.062770Z","iopub.status.idle":"2024-09-24T14:19:33.336632Z","shell.execute_reply":"2024-09-24T14:19:33.335573Z","shell.execute_reply.started":"2024-09-24T12:28:05.063385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1, Loss: 0.3539914568386834\n","Epoch 2, Loss: 0.15147645264191842\n","Epoch 3, Loss: 0.13129001925893066\n","Test Accuracy on CIFAR-10: 95.22%\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","model.heads.head = nn.Linear(model.heads.head.in_features, 10) \n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","for param in model.heads.parameters():\n","    param.requires_grad = True\n","\n","transform = transforms.Compose([\n","    transforms.Resize((384, 384)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(model.heads.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T14:57:49.735770Z","iopub.status.busy":"2024-09-24T14:57:49.734931Z","iopub.status.idle":"2024-09-24T14:58:13.992054Z","shell.execute_reply":"2024-09-24T14:58:13.990751Z","shell.execute_reply.started":"2024-09-24T14:57:49.735728Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0e946612c404080a84e3f56659fef69","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"168e34f3c8b14ee6939d7df3b14d1646","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31712de3077f45c2831c12ac0931d93e","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1e870dca9f04de0b75e9946f9903592","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8b131c6157b4561be75b9135981d047","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d783fb170ec42c196028a7129603f76","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e4dcd5046ee49548983eb8e930c64a8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cde8b249e8754dc196604331e6d28ed6","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n","outputs = model(**inputs)\n","logits_per_image = outputs.logits_per_image \n","probs = logits_per_image.softmax(dim=1) "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T14:58:18.505014Z","iopub.status.busy":"2024-09-24T14:58:18.504334Z","iopub.status.idle":"2024-09-24T15:00:43.463309Z","shell.execute_reply":"2024-09-24T15:00:43.462224Z","shell.execute_reply.started":"2024-09-24T14:58:18.504973Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","CLIP model accuracy on CIFAR-10 test set: 87.30%\n"]}],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  \n","    transforms.ToTensor(),         \n","])\n","\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","cifar10_labels = [\n","    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n","    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n","]\n","\n","\n","model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        pil_images = [Image.fromarray((image.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8')) for image in images]\n","        image_inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n","        \n","        text_inputs = processor(text=cifar10_labels, return_tensors=\"pt\", padding=True).to(device)\n","        \n","        outputs = model(**image_inputs, **text_inputs)\n","        \n","        logits_per_image = outputs.logits_per_image\n","        \n","        probs = logits_per_image.softmax(dim=1)\n","        \n","        _, predicted = torch.max(probs, dim=1)\n","        \n","        correct += (predicted == labels.to(device)).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = 100 * correct / total\n","print(f'CLIP model accuracy on CIFAR-10 test set: {accuracy:.2f}%')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
