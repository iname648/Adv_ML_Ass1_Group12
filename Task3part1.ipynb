{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:44:00.281889Z","iopub.status.busy":"2024-09-28T17:44:00.281572Z","iopub.status.idle":"2024-09-28T17:44:06.818673Z","shell.execute_reply":"2024-09-28T17:44:06.817871Z","shell.execute_reply.started":"2024-09-28T17:44:00.281856Z"},"id":"B5t37oJ-L913","trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn as nn\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:54:05.635334Z","iopub.status.busy":"2024-09-28T17:54:05.634899Z","iopub.status.idle":"2024-09-28T17:54:05.659622Z","shell.execute_reply":"2024-09-28T17:54:05.658625Z","shell.execute_reply.started":"2024-09-28T17:54:05.635287Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['dct2_images']\n"]}],"source":["import os\n","\n","dataset_path = '/kaggle/input/pacs-dataset/dct2_images'\n","print(os.listdir(dataset_path))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-28T18:41:36.848798Z","iopub.status.busy":"2024-09-28T18:41:36.847928Z","iopub.status.idle":"2024-09-28T18:41:36.992269Z","shell.execute_reply":"2024-09-28T18:41:36.991147Z","shell.execute_reply.started":"2024-09-28T18:41:36.848753Z"},"id":"oqvwDgfWL915","outputId":"78e9d201-26a2-4b95-b37c-3ecf4bf34853","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current Directory: /kaggle/working\n","Contents of the current directory: ['.virtual_documents']\n","Dataset found at /kaggle/input/pacs-dataset/dct2_images\n","Loaded 9991 images from PACS dataset.\n"]}],"source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","transform_pacs = transforms.Compose([\n","    transforms.Resize((224, 224)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","current_directory = os.getcwd()\n","\n","print(\"Current Directory:\", current_directory)\n","\n","print(\"Contents of the current directory:\", os.listdir(current_directory))\n","\n","dataset_path = '/kaggle/input/pacs-dataset/dct2_images'\n","\n","if os.path.exists(dataset_path):\n","    print(f\"Dataset found at {dataset_path}\")\n","else:\n","    print(f\"Dataset not found at {dataset_path}. Please verify the path.\")\n","\n","pacs_dataset = datasets.ImageFolder(root=dataset_path, transform=transform_pacs)\n","pacs_loader = DataLoader(pacs_dataset, batch_size=32, shuffle=True)\n","\n","print(f\"Loaded {len(pacs_dataset)} images from PACS dataset.\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T17:40:05.347644Z","iopub.status.busy":"2024-09-26T17:40:05.347308Z","iopub.status.idle":"2024-09-26T17:41:53.534197Z","shell.execute_reply":"2024-09-26T17:41:53.532980Z","shell.execute_reply.started":"2024-09-26T17:40:05.347612Z"},"id":"gJCxnHKLL916","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EfficientNet PACS Accuracy: 72.14%\n"]}],"source":["effnet_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n","effnet_model.classifier[1] = nn.Linear(effnet_model.classifier[1].in_features, len(pacs_dataset.classes))  \n","\n","for param in effnet_model.features.parameters():\n","    param.requires_grad = False\n","\n","effnet_model.to(device)\n","\n","optimizer_effnet = optim.AdamW(effnet_model.classifier.parameters(), lr=1e-4)\n","criterion_effnet = nn.CrossEntropyLoss()\n","\n","def train_effnet(model, loader, optimizer, criterion):\n","    model.train()\n","    for images, labels in loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","def evaluate_effnet(model, loader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f'EfficientNet PACS Accuracy: {accuracy:.2f}%')\n","\n","train_effnet(effnet_model, pacs_loader, optimizer_effnet, criterion_effnet)\n","evaluate_effnet(effnet_model, pacs_loader)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T17:41:53.536782Z","iopub.status.busy":"2024-09-26T17:41:53.535929Z","iopub.status.idle":"2024-09-26T18:04:22.997279Z","shell.execute_reply":"2024-09-26T18:04:22.996129Z","shell.execute_reply.started":"2024-09-26T17:41:53.536730Z"},"id":"27Nz5bR0L916","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ViT PACS Accuracy: 75.36%\n"]}],"source":["vit_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","vit_model.heads.head = nn.Linear(vit_model.heads.head.in_features, len(pacs_dataset.classes))  \n","\n","for param in vit_model.encoder.parameters():\n","    param.requires_grad = False\n","\n","vit_model.to(device)\n","\n","optimizer_vit = optim.AdamW(vit_model.heads.parameters(), lr=1e-4)\n","criterion_vit = nn.CrossEntropyLoss()\n","\n","def train_vit(model, loader, optimizer, criterion):\n","    model.train()\n","    for images, labels in loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        images = transforms.Resize((384, 384))(images)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","def evaluate_vit(model, loader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            images = transforms.Resize((384, 384))(images)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f'ViT PACS Accuracy: {accuracy:.2f}%')\n","\n","train_vit(vit_model, pacs_loader, optimizer_vit, criterion_vit)\n","evaluate_vit(vit_model, pacs_loader)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T18:41:43.108893Z","iopub.status.busy":"2024-09-28T18:41:43.108506Z","iopub.status.idle":"2024-09-28T18:44:00.496319Z","shell.execute_reply":"2024-09-28T18:44:00.495250Z","shell.execute_reply.started":"2024-09-28T18:41:43.108857Z"},"id":"nhLYzyE-L916","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CLIP PACS Accuracy: 14.75%\n"]}],"source":["from PIL import Image\n","import torch\n","from transformers import CLIPModel, CLIPProcessor\n","\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","clip_model.to(device)\n","\n","class_labels = ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']\n","\n","\n","def evaluate_clip(model, processor, loader, class_labels):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, targets in loader:\n","            pil_images = [Image.fromarray((image.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8')) for image in images]\n","            \n","            image_inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n","            text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n","            \n","            outputs = model(**image_inputs, **text_inputs)\n","            \n","            logits_per_image = outputs.logits_per_image\n","            probs = logits_per_image.softmax(dim=1)\n","            \n","            _, predicted = torch.max(probs, 1)\n","            \n","            correct += (predicted.cpu() == targets).sum().item()\n","            total += images.size(0)\n","    \n","    accuracy = 100 * correct / total\n","    print(f'CLIP PACS Accuracy: {accuracy:.2f}%')\n","\n","evaluate_clip(clip_model, clip_processor, pacs_loader, class_labels)\n"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1733444,"sourceId":2834375,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
