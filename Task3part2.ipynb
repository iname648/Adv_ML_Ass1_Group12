{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T21:08:40.257007Z","iopub.status.busy":"2024-09-28T21:08:40.256615Z","iopub.status.idle":"2024-09-28T21:08:46.448458Z","shell.execute_reply":"2024-09-28T21:08:46.447646Z","shell.execute_reply.started":"2024-09-28T21:08:40.256963Z"},"id":"B5t37oJ-L913","trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn as nn\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-28T21:09:23.416130Z","iopub.status.busy":"2024-09-28T21:09:23.415748Z","iopub.status.idle":"2024-09-28T21:09:23.421977Z","shell.execute_reply":"2024-09-28T21:09:23.420984Z","shell.execute_reply.started":"2024-09-28T21:09:23.416087Z"},"id":"oqvwDgfWL915","outputId":"78e9d201-26a2-4b95-b37c-3ecf4bf34853","trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","transform_svhn = transforms.Compose([\n","    transforms.Resize((64, 64)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  \n","   \n","])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-28T21:34:08.478869Z","iopub.status.busy":"2024-09-28T21:34:08.478144Z","iopub.status.idle":"2024-09-28T21:34:11.587488Z","shell.execute_reply":"2024-09-28T21:34:11.586654Z","shell.execute_reply.started":"2024-09-28T21:34:08.478827Z"},"id":"-TN0QeibL915","outputId":"f97dac9d-9956-431a-d9e3-1facbc83e01f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: ./data/train_32x32.mat\n","Using downloaded and verified file: ./data/test_32x32.mat\n"]}],"source":["svhn_train = datasets.SVHN(root='./data', split='train', download=True, transform=transform_svhn)\n","svhn_test = datasets.SVHN(root='./data', split='test', download=True, transform=transform_svhn)\n","\n","train_loader = DataLoader(svhn_train, batch_size=32, shuffle=True)\n","test_loader = DataLoader(svhn_test, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T21:34:16.439748Z","iopub.status.busy":"2024-09-28T21:34:16.439354Z","iopub.status.idle":"2024-09-28T21:34:16.940013Z","shell.execute_reply":"2024-09-28T21:34:16.939137Z","shell.execute_reply.started":"2024-09-28T21:34:16.439712Z"},"id":"gJCxnHKLL916","trusted":true},"outputs":[],"source":["effnet_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n","\n","num_ftrs = effnet_model.classifier[1].in_features\n","effnet_model.classifier = nn.Sequential(\n","     nn.Dropout(p=0.5, inplace=True),\n","    nn.Linear(num_ftrs, 10),  \n",")\n","\n","effnet_model = effnet_model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(effnet_model.parameters(), lr=0.0001)\n","\n","def train_effnet(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for images, labels in loader:\n","        images, labels = images.to(device), labels.to(device)  \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Training Loss: {total_loss / len(loader):.4f}\")\n","\n","def evaluate_effnet(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f'EfficientNet SVHN Accuracy: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T21:34:22.685412Z","iopub.status.busy":"2024-09-28T21:34:22.684991Z","iopub.status.idle":"2024-09-28T21:38:08.528149Z","shell.execute_reply":"2024-09-28T21:38:08.527177Z","shell.execute_reply.started":"2024-09-28T21:34:22.685367Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Loss: 1.7701\n","EfficientNet SVHN Accuracy: 60.89%\n"]}],"source":["train_effnet(effnet_model, train_loader, optimizer, criterion, device)\n","evaluate_effnet(effnet_model, test_loader, device)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T05:52:18.734262Z","iopub.status.busy":"2024-09-27T05:52:18.733209Z","iopub.status.idle":"2024-09-27T05:52:31.836155Z","shell.execute_reply":"2024-09-27T05:52:31.835283Z","shell.execute_reply.started":"2024-09-27T05:52:18.734192Z"},"id":"27Nz5bR0L916","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: ./data/train_32x32.mat\n","Using downloaded and verified file: ./data/test_32x32.mat\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n","100%|██████████| 331M/331M [00:08<00:00, 42.6MB/s] \n"]}],"source":["transform_svhn = transforms.Compose([\n","    transforms.Resize((384, 384)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),  \n","])\n","svhn_train = datasets.SVHN(root='./data', split='train', download=True, transform=transform_svhn)\n","svhn_test = datasets.SVHN(root='./data', split='test', download=True, transform=transform_svhn)\n","\n","train_loader = DataLoader(svhn_train, batch_size=32, shuffle=True)\n","test_loader = DataLoader(svhn_test, batch_size=32, shuffle=False)\n","\n","vit_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","for param in vit_model.parameters():\n","    param.requires_grad = False\n","    \n","    \n","for param in vit_model.heads.head.parameters():\n","    param.requires_grad = True\n","\n","num_ftrs = vit_model.heads.head.in_features\n","vit_model.heads.head = nn.Linear(num_ftrs, 10)  \n","\n","vit_model = vit_model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vit_model.parameters(), lr=1e-4)\n","\n","\n","def train_vit(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for images, labels in loader:\n","        images, labels = images.to(device), labels.to(device)  \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Training Loss: {total_loss / len(loader):.4f}\")\n","\n","def evaluate_vit(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f'ViT SVHN Accuracy: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T05:52:31.838028Z","iopub.status.busy":"2024-09-27T05:52:31.837596Z","iopub.status.idle":"2024-09-27T07:02:32.852076Z","shell.execute_reply":"2024-09-27T07:02:32.851119Z","shell.execute_reply.started":"2024-09-27T05:52:31.837980Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Loss: 1.7791\n","ViT SVHN Accuracy: 50.87%\n"]}],"source":["train_vit(vit_model, train_loader, optimizer, criterion, device)\n","evaluate_vit(vit_model, test_loader, device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T05:45:35.908899Z","iopub.status.busy":"2024-09-27T05:45:35.907761Z","iopub.status.idle":"2024-09-27T05:52:18.730896Z","shell.execute_reply":"2024-09-27T05:52:18.729827Z","shell.execute_reply.started":"2024-09-27T05:45:35.908842Z"},"id":"nhLYzyE-L916","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"673eb2bc42bc4794bfdb9ac2810aef54","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21994444749e4889baa70a91c0e7309d","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c52d798ff3dd4ddd98c4d80c1494b2d8","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2bb8a94739141d4b0744e672e432013","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac3a1c618b9e41039cc58291b2245f1c","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6110ac73ba54cd7a1f9f9c74d4c02f9","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1cd2119a8864046a908a5e07d10686f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b0a798a6a8f46b7a1b76cc1b279ccae","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["CLIP SVHN Accuracy: 24.20%\n"]}],"source":["from PIL import Image\n","import torch\n","from transformers import CLIPModel, CLIPProcessor\n","\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","clip_model.to(device)\n","\n","class_labels = [str(i) for i in range(10)]  \n","\n","def evaluate_clip(model, processor, loader, class_labels):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, targets in loader:\n","            pil_images = [Image.fromarray((image.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8')) for image in images]\n","            \n","            image_inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n","            text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n","            \n","            outputs = model(**image_inputs, **text_inputs)\n","            \n","            logits_per_image = outputs.logits_per_image\n","            probs = logits_per_image.softmax(dim=1)\n","            \n","            _, predicted = torch.max(probs, 1)\n","            \n","            correct += (predicted.cpu() == targets).sum().item()\n","            total += images.size(0)\n","    \n","    accuracy = 100 * correct / total\n","    print(f'CLIP SVHN Accuracy: {accuracy:.2f}%')\n","\n","evaluate_clip(clip_model, clip_processor, test_loader, class_labels)\n"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5770295,"sourceId":9485408,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
