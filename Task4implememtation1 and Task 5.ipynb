{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-29T14:17:25.004983Z","iopub.status.busy":"2024-09-29T14:17:25.004192Z","iopub.status.idle":"2024-09-29T14:17:33.446642Z","shell.execute_reply":"2024-09-29T14:17:33.445755Z","shell.execute_reply.started":"2024-09-29T14:17:25.004921Z"},"id":"vgUZIEPObluk","outputId":"77461249-0e61-4d58-b6d1-345ceee094cc","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n","100%|██████████| 74.5M/74.5M [00:00<00:00, 115MB/s] \n"]}],"source":["import torch\n","from torchvision import models\n","\n","model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","model.eval()\n","\n","weights = models.EfficientNet_B4_Weights.DEFAULT.get_state_dict(progress=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"execution":{"iopub.execute_input":"2024-09-29T14:17:40.072931Z","iopub.status.busy":"2024-09-29T14:17:40.071903Z","iopub.status.idle":"2024-09-29T14:27:27.876759Z","shell.execute_reply":"2024-09-29T14:27:27.875781Z","shell.execute_reply.started":"2024-09-29T14:17:40.072889Z"},"id":"VEC21sBAblum","outputId":"6ca28be6-b0ee-4c6f-da05-1327001ac14b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:11<00:00, 14631524.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1, Loss: 1.6568229990896322\n","Epoch 2, Loss: 1.0595059520299817\n","Epoch 3, Loss: 0.866766551520225\n","Test Accuracy on CIFAR-10: 80.02%\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","efficientnet_b4_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n","\n","efficientnet_b4_model.classifier[1] = nn.Linear(efficientnet_b4_model.classifier[1].in_features, 10)  \n","\n","for param in efficientnet_b4_model.features.parameters():\n","    param.requires_grad = False\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(efficientnet_b4_model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","efficientnet_b4_model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    efficientnet_b4_model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = efficientnet_b4_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","efficientnet_b4_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = efficientnet_b4_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-29T14:30:26.375888Z","iopub.status.busy":"2024-09-29T14:30:26.375376Z","iopub.status.idle":"2024-09-29T14:30:47.830168Z","shell.execute_reply":"2024-09-29T14:30:47.829300Z","shell.execute_reply.started":"2024-09-29T14:30:26.375847Z"},"id":"l5SIS2J7blun","outputId":"eddffa39-0c65-4b8d-f1ab-38f95d7b0a32","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n","100%|██████████| 331M/331M [00:18<00:00, 18.4MB/s] \n"]}],"source":["import torch\n","from torchvision import models\n","\n","vit_b16_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","vit_b16_model.eval()\n","\n","weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.get_state_dict(progress=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"execution":{"iopub.execute_input":"2024-09-29T14:31:00.041339Z","iopub.status.busy":"2024-09-29T14:31:00.040673Z","iopub.status.idle":"2024-09-29T15:35:06.943549Z","shell.execute_reply":"2024-09-29T15:35:06.942553Z","shell.execute_reply.started":"2024-09-29T14:31:00.041296Z"},"id":"P-o1G5-kbluo","outputId":"eb3bcb61-0f2e-4346-bb54-057c138981a6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1, Loss: 0.3775843938756603\n","Epoch 2, Loss: 0.15415096293565217\n","Epoch 3, Loss: 0.13294663082379835\n","Test Accuracy on CIFAR-10: 95.16%\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","vit_b16_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n","\n","vit_b16_model.heads.head = nn.Linear(vit_b16_model.heads.head.in_features, 10) \n","\n","for param in vit_b16_model.parameters():\n","    param.requires_grad = False\n","\n","for param in vit_b16_model.heads.parameters():\n","    param.requires_grad = True\n","\n","transform = transforms.Compose([\n","    transforms.Resize((384, 384)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","optimizer = optim.AdamW(vit_b16_model.heads.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vit_b16_model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    vit_b16_model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = vit_b16_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","vit_b16_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = vit_b16_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Test Accuracy on CIFAR-10: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536,"referenced_widgets":["11c7a5e852664da5a5c055567ecb4e5b","39aa549936e943d683baa01ae56cbb5b","ec01489863174cc8bd538773071861fa","8d6df37098ed4cacba31057f75ca2f27","c76f3164643743398e7cc33df62d07ff","6f37165e4f8949198fbd374d2d292b6c","fef9f6ffd0504be88d49a7dafab5256f","f866c2b81d8644d3ab914d2b85dd2347","63046fc17f3c4f48bb33e6c9e6a06c35","b688d48c56e048528938c1823559ac0a","314765fc6d594e7cb915b9daee1e77c3","bcb8da82efee401ab9dd12002f7bc99c","33c6819a5b5746729431656651d5d389","233f1b5f44824ee2bb96b3206754d2a3","e52bf05cac1a48fc8c864f7d6a141a93","747dc4a15ee74b458d4f7ba3877b8c0b","7d18d321a83644b7948cc4cf8e05c427","4f9294c5440e4a1d9a5c6bb3b89ce1a9","cb3ced2ed6014469b7493400b22664bd","a014e3a6f80a4172a9625239b6c3688b","6fd4c5a0539e41edb8d9fd52e5f628e6","73a5eccf58634583b15dbf554c985f68"]},"execution":{"iopub.execute_input":"2024-09-28T15:18:44.814870Z","iopub.status.busy":"2024-09-28T15:18:44.813997Z","iopub.status.idle":"2024-09-28T15:19:05.301232Z","shell.execute_reply":"2024-09-28T15:19:05.300330Z","shell.execute_reply.started":"2024-09-28T15:18:44.814828Z"},"id":"FeHSMMS6bluo","outputId":"8b4f4eab-5596-4b5f-9ada-3392279114bf","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a131d2c00264941a91423471c9ea1e1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95438489314446f4ac5755e642c704bf","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df4bb15e28d24a788487b22b08af988a","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"969ec74c088940199520afe101aa1a97","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e506d6d87af34b2081a8fb2b492e6e9b","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"333dcac2ecdd40798b957aa4a5755629","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4057c960a13f413e95b48ca6c40e1dce","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b39b8cc7caa4d1cb18bd9d0f9067bf1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n","outputs = clip_model(**inputs)\n","logits_per_image = outputs.logits_per_image \n","probs = logits_per_image.softmax(dim=1) "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382,"referenced_widgets":["d4f742a1b724435d8170ecd873cfa623","f9189d84940f4b55be028bf2cf4e2851","d0734d0d730e44feaefe3738faf99046","350b50a272db45bd819d173a2cf23913","8e3756de2bf947c284e02d5227198d6e","1e4985e85f244dd9950445214a2d69e8","e82582a563864749b9780ca7b8a40a72","d0815afbacef4d51918980d3ccb754e7","8500995809c844a9a90f928fa6e7dcdc","539efaf8f3e24b5bb798a236a3d7f0a8","801d74a454184088b230472ca66abde9"]},"execution":{"iopub.execute_input":"2024-09-28T15:19:11.902488Z","iopub.status.busy":"2024-09-28T15:19:11.901804Z","iopub.status.idle":"2024-09-28T15:20:54.306945Z","shell.execute_reply":"2024-09-28T15:20:54.305648Z","shell.execute_reply.started":"2024-09-28T15:19:11.902438Z"},"id":"DkRJLOabbluo","outputId":"66cc34a4-21a1-4958-b836-6f891b557647","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","CLIP clip_model accuracy on CIFAR-10 test set: 87.30%\n"]}],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","clip_model.to(device)\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  \n","    transforms.ToTensor(),        \n","])\n","\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","cifar10_labels = [\n","    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n","    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n","]\n","\n","\n","clip_model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","       \n","        pil_images = [Image.fromarray((image.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8')) for image in images]\n","        image_inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n","\n","        text_inputs = processor(text=cifar10_labels, return_tensors=\"pt\", padding=True).to(device)\n","\n","        outputs = clip_model(**image_inputs, **text_inputs)\n","\n","        logits_per_image = outputs.logits_per_image\n","\n","        probs = logits_per_image.softmax(dim=1)\n","\n","        _, predicted = torch.max(probs, dim=1)\n","\n","        correct += (predicted == labels.to(device)).sum().item()\n","        total += labels.size(0)\n","  \n","accuracy = 100 * correct / total\n","print(f'CLIP clip_model accuracy on CIFAR-10 test set: {accuracy:.2f}%')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:12:48.565984Z","iopub.status.busy":"2024-09-27T18:12:48.565575Z","iopub.status.idle":"2024-09-27T19:51:32.291461Z","shell.execute_reply":"2024-09-27T19:51:32.290263Z","shell.execute_reply.started":"2024-09-27T18:12:48.565950Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","ViT_B_16 Shape Bias Accuracy: 24.46%\n","ViT_B_16 Texture Bias Accuracy: 95.88%\n","ViT_B_16 Color Bias Accuracy: 79.93%\n","EfficientNet_B4 Shape Bias Accuracy: 10.12%\n","EfficientNet_B4 Texture Bias Accuracy: 10.03%\n","EfficientNet_B4 Color Bias Accuracy: 43.23%\n"]}],"source":["import os\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","shape_transform = transforms.Compose([\n","    transforms.Resize((384, 384)),\n","    transforms.Grayscale(num_output_channels=3),  \n","    transforms.ToTensor(), \n","    transforms.Lambda(lambda x: torch.where(x > 0.5, torch.tensor(1.0, device=x.device), torch.tensor(0.0, device=x.device))) \n","])\n","\n","\n","texture_transform = transforms.Compose([\n","    transforms.Resize((384, 384)),  \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n","    transforms.Lambda(lambda x: x + 0.1 * torch.randn_like(x, device=x.device))  \n","])\n","\n","\n","color_transform = transforms.Compose([\n","    transforms.Resize((384, 384)),  \n","    transforms.Grayscale(num_output_channels=1), \n","    transforms.ToTensor(), \n","    transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0)), \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","\n","\n","cifar10_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=None)\n","cifar10_test = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=None)\n","\n","cifar10_data = cifar10_train + cifar10_test\n","\n","def collate_fn(batch, transform):\n","    images, labels = zip(*batch)\n","    transformed_images = [transform(img) for img in images]\n","    return torch.stack(transformed_images), torch.tensor(labels)\n","\n","shape_loader = DataLoader(cifar10_data, batch_size=32, shuffle=False, collate_fn=lambda batch: collate_fn(batch, shape_transform))\n","texture_loader = DataLoader(cifar10_data, batch_size=32, shuffle=False, collate_fn=lambda batch: collate_fn(batch, texture_transform))\n","color_loader = DataLoader(cifar10_data, batch_size=32, shuffle=False, collate_fn=lambda batch: collate_fn(batch, color_transform))\n","\n","vit_b16_model = vit_b16_model.to(device)\n","efficientnet_b4_model = efficientnet_b4_model.to(device)\n","\n","def evaluate_model(model, data_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device) \n","            outputs = model(images)  \n","            _, predicted = torch.max(outputs, 1)  \n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","shape_bias_acc_vit = evaluate_model(vit_b16_model, shape_loader, device)\n","texture_bias_acc_vit = evaluate_model(vit_b16_model, texture_loader, device)\n","color_bias_acc_vit = evaluate_model(vit_b16_model, color_loader, device)\n","\n","shape_bias_acc_effnet = evaluate_model(efficientnet_b4_model, shape_loader, device)\n","texture_bias_acc_effnet = evaluate_model(efficientnet_b4_model, texture_loader, device)\n","color_bias_acc_effnet = evaluate_model(efficientnet_b4_model, color_loader, device)\n","\n","print(f'ViT_B_16 Shape Bias Accuracy: {shape_bias_acc_vit:.2f}%')\n","print(f'ViT_B_16 Texture Bias Accuracy: {texture_bias_acc_vit:.2f}%')\n","print(f'ViT_B_16 Color Bias Accuracy: {color_bias_acc_vit:.2f}%')\n","\n","print(f'EfficientNet_B4 Shape Bias Accuracy: {shape_bias_acc_effnet:.2f}%')\n","print(f'EfficientNet_B4 Texture Bias Accuracy: {texture_bias_acc_effnet:.2f}%')\n","print(f'EfficientNet_B4 Color Bias Accuracy: {color_bias_acc_effnet:.2f}%')\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T16:14:21.659652Z","iopub.status.busy":"2024-09-29T16:14:21.658965Z","iopub.status.idle":"2024-09-29T16:16:15.299581Z","shell.execute_reply":"2024-09-29T16:16:15.298787Z","shell.execute_reply.started":"2024-09-29T16:14:21.659612Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa447b903db94e5582bad09d951e8b2d","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/5640 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving shape bias dataset with edge detection method...\n","Saving texture bias dataset with DTD textures...\n","Saving color bias dataset...\n","Datasets saved successfully!\n"]}],"source":["import os\n","import torch\n","from torchvision import datasets, transforms\n","import torchvision.transforms.functional as F\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import random\n","from datasets import load_dataset\n","\n","shape_dir = './data/shape'\n","texture_dir = './data/texture'\n","color_dir = './data/color'\n","\n","os.makedirs(shape_dir, exist_ok=True)\n","os.makedirs(texture_dir, exist_ok=True)\n","os.makedirs(color_dir, exist_ok=True)\n","\n","transform = transforms.Compose([transforms.Resize((384, 384)), transforms.ToTensor()])\n","cifar10_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n","cifar10_test = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n","\n","cifar10_data = cifar10_train + cifar10_test\n","\n","def edge_transform(img):\n","    img = np.array(img) \n","    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n","    edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)  \n","    return Image.fromarray(edges)  \n","\n","shape_transform = transforms.Compose([\n","    transforms.Lambda(lambda img: edge_transform(img)),  \n","    transforms.ToTensor() \n","])\n","\n","texture_dataset = load_dataset(\"cansa/Describable-Textures-Dataset-DTD\", split=\"train\")\n","\n","def get_texture_images(dataset):\n","    textures = []\n","    for item in dataset:\n","        img = item['image'] if isinstance(item['image'], Image.Image) else Image.open(item['image']).convert('RGB')\n","        textures.append(transforms.Resize((384, 384))(img))  \n","    return textures\n","\n","texture_images = get_texture_images(texture_dataset)\n","\n","def blend_with_texture(img, texture):\n","    alpha = 0.5  \n","    img_tensor = transforms.ToTensor()(img) if not isinstance(img, torch.Tensor) else img\n","    texture_tensor = transforms.ToTensor()(texture) if not isinstance(texture, torch.Tensor) else texture\n","    \n","    if img_tensor.size() != texture_tensor.size():\n","        texture_tensor = F.resize(texture_tensor, img_tensor.size()[1:]) \n","\n","    blended_img = (alpha * img_tensor) + ((1 - alpha) * texture_tensor)\n","    return blended_img\n","\n","def apply_texture_transform(img):\n","    texture_img = random.choice(texture_images)  \n","    return blend_with_texture(img, texture_img)  \n","\n","texture_transform = transforms.Compose([\n","    transforms.ToTensor(), \n","    transforms.Lambda(lambda x: apply_texture_transform(x)),  \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","color_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3), \n","    transforms.Resize((224, 224)), \n","    transforms.ToTensor(),  \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","def save_images_limited(dataset, transform, save_dir, limit=800):\n","    os.makedirs(save_dir, exist_ok=True)\n","    for i, (img, label) in enumerate(dataset):\n","        if i >= limit:  \n","            break\n","        img_pil = transforms.ToPILImage()(img) \n","        img_transformed = transform(img_pil) \n","        img_pil_transformed = transforms.ToPILImage()(img_transformed)  \n","        class_dir = os.path.join(save_dir, str(label))\n","        os.makedirs(class_dir, exist_ok=True)\n","        img_pil_transformed.save(os.path.join(class_dir, f\"{i}.png\"))\n","\n","print(\"Saving shape bias dataset with edge detection method...\")\n","save_images_limited(cifar10_data, shape_transform, shape_dir)\n","\n","print(\"Saving texture bias dataset with DTD textures...\")\n","save_images_limited(cifar10_data, texture_transform, texture_dir)\n","\n","print(\"Saving color bias dataset...\")\n","save_images_limited(cifar10_data, color_transform, color_dir)\n","\n","print(\"Datasets saved successfully!\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T16:18:03.971093Z","iopub.status.busy":"2024-09-29T16:18:03.970338Z","iopub.status.idle":"2024-09-29T16:18:23.097703Z","shell.execute_reply":"2024-09-29T16:18:23.095553Z","shell.execute_reply.started":"2024-09-29T16:18:03.971053Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape Bias Accuracy: 0.0000\n"]},{"ename":"RecursionError","evalue":"maximum recursion depth exceeded while calling a Python object","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:263\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 263\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3427\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3426\u001b[0m filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/_util.py:10\u001b[0m, in \u001b[0;36mis_path\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_path\u001b[39m(f: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TypeGuard[StrOrBytesPath]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPathLike\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in __instancecheck__","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m shape_bias_accuracy \u001b[38;5;241m=\u001b[39m evaluate_bias(shape_loader)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape Bias Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape_bias_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m texture_bias_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexture_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTexture Bias Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtexture_bias_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m color_bias_accuracy \u001b[38;5;241m=\u001b[39m evaluate_bias(color_loader)\n","Cell \u001b[0;32mIn[20], line 80\u001b[0m, in \u001b[0;36mevaluate_bias\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     78\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     81\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     82\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:479\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)  \u001b[38;5;66;03m# Blend with CIFAR-10 image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Texture transform using real textures\u001b[39;00m\n\u001b[1;32m     45\u001b[0m texture_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     46\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m---> 47\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mapply_texture_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m),  \u001b[38;5;66;03m# Apply texture blending\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Color Bias: Convert to grayscale, duplicate to RGB, and apply normalization\u001b[39;00m\n\u001b[1;32m     53\u001b[0m color_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     54\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),  \u001b[38;5;66;03m# Convert to grayscale and create 3 channels\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert PIL image to tensor\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     58\u001b[0m ])\n","Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mapply_texture_transform\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_texture_transform\u001b[39m(img):\n\u001b[0;32m---> 40\u001b[0m     texture_img, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexture_dataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose a random texture\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     texture_resized \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))(texture_img)  \u001b[38;5;66;03m# Resize texture to match ViT input\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)\n","File \u001b[0;32m/opt/conda/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:479\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)  \u001b[38;5;66;03m# Blend with CIFAR-10 image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Texture transform using real textures\u001b[39;00m\n\u001b[1;32m     45\u001b[0m texture_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     46\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m---> 47\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mapply_texture_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m),  \u001b[38;5;66;03m# Apply texture blending\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Color Bias: Convert to grayscale, duplicate to RGB, and apply normalization\u001b[39;00m\n\u001b[1;32m     53\u001b[0m color_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     54\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),  \u001b[38;5;66;03m# Convert to grayscale and create 3 channels\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert PIL image to tensor\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     58\u001b[0m ])\n","Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mapply_texture_transform\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_texture_transform\u001b[39m(img):\n\u001b[0;32m---> 40\u001b[0m     texture_img, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexture_dataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose a random texture\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     texture_resized \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))(texture_img)  \u001b[38;5;66;03m# Resize texture to match ViT input\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)\n","File \u001b[0;32m/opt/conda/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n","    \u001b[0;31m[... skipping similar frames: <lambda> at line 47 (367 times), Compose.__call__ at line 95 (367 times), Lambda.__call__ at line 479 (367 times), DatasetFolder.__getitem__ at line 247 (367 times), apply_texture_transform at line 40 (367 times), Random.choice at line 378 (367 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:479\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)  \u001b[38;5;66;03m# Blend with CIFAR-10 image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Texture transform using real textures\u001b[39;00m\n\u001b[1;32m     45\u001b[0m texture_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     46\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m---> 47\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mapply_texture_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m),  \u001b[38;5;66;03m# Apply texture blending\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Color Bias: Convert to grayscale, duplicate to RGB, and apply normalization\u001b[39;00m\n\u001b[1;32m     53\u001b[0m color_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     54\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),  \u001b[38;5;66;03m# Convert to grayscale and create 3 channels\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to 224x224 for ViT\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert PIL image to tensor\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Standard normalization\u001b[39;00m\n\u001b[1;32m     58\u001b[0m ])\n","Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mapply_texture_transform\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_texture_transform\u001b[39m(img):\n\u001b[0;32m---> 40\u001b[0m     texture_img, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexture_dataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose a random texture\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     texture_resized \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))(texture_img)  \u001b[38;5;66;03m# Resize texture to match ViT input\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blend_with_texture(img, texture_resized)\n","File \u001b[0;32m/opt/conda/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.optim as optim\n","from torchvision import models\n","import numpy as np\n","import random\n","import cv2\n","from PIL import Image\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","def edge_transform(img):\n","    img = np.array(img)  \n","    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n","    edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)  \n","    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)  \n","    return Image.fromarray(edges_rgb) \n","\n","shape_transform = transforms.Compose([\n","    transforms.Lambda(lambda img: edge_transform(img)), \n","    transforms.Resize((224, 224)), \n","    transforms.ToTensor(),  \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","texture_dataset_dir = './data/texture'\n","texture_dataset = datasets.ImageFolder(root=texture_dataset_dir, transform=transforms.ToTensor())\n","\n","texture_images = [texture_dataset[i][0] for i in range(len(texture_dataset))]\n","\n","def apply_texture_transform(img):\n","    texture_img = random.choice(texture_images)  \n","    texture_resized = transforms.Resize((224, 224))(texture_img) \n","    return blend_with_texture(img, texture_resized)\n","\n","texture_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Lambda(lambda x: apply_texture_transform(x)), \n","    transforms.Resize((224, 224)),  \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","color_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3), \n","    transforms.Resize((224, 224)), \n","    transforms.ToTensor(),  \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n","])\n","\n","shape_dataset = datasets.ImageFolder(root='./data/shape', transform=shape_transform)\n","texture_dataset = datasets.ImageFolder(root='./data/texture', transform=texture_transform)\n","color_dataset = datasets.ImageFolder(root='./data/color', transform=color_transform)\n","\n","batch_size = 32\n","shape_loader = DataLoader(shape_dataset, batch_size=batch_size, shuffle=True)\n","texture_loader = DataLoader(texture_dataset, batch_size=batch_size, shuffle=True)\n","color_loader = DataLoader(color_dataset, batch_size=batch_size, shuffle=True)\n","\n","model = models.vit_b_16(pretrained=True).to(device)\n","model.eval()  \n","\n","def evaluate_bias(data_loader):\n","    total = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels.to(device)).sum().item()\n","    return correct / total\n","\n","shape_bias_accuracy = evaluate_bias(shape_loader)\n","print(f'Shape Bias Accuracy: {shape_bias_accuracy:.4f}')\n","\n","texture_bias_accuracy = evaluate_bias(texture_loader)\n","print(f'Texture Bias Accuracy: {texture_bias_accuracy:.4f}')\n","\n","color_bias_accuracy = evaluate_bias(color_loader)\n","print(f'Color Bias Accuracy: {color_bias_accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:36:21.830823Z","iopub.status.busy":"2024-09-28T15:36:21.830413Z","iopub.status.idle":"2024-09-28T15:36:22.512048Z","shell.execute_reply":"2024-09-28T15:36:22.511279Z","shell.execute_reply.started":"2024-09-28T15:36:21.830778Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]}],"source":["transform = transforms.Compose([\n","  transforms.Resize((384, 384)), \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","cifar_loader = DataLoader(cifar10_test, batch_size=32, shuffle=False)\n","\n","cifar10_labels = [\n","    'airplane', 'automobile', 'bird', 'cat', 'deer',\n","    'dog', 'frog', 'horse', 'ship', 'truck'\n","]\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","efficient_net = efficientnet_b4_model\n","vit_model = vit_b16_model\n","\n","clip_model = clip_model\n","clip_processor = processor"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:36:26.702788Z","iopub.status.busy":"2024-09-28T15:36:26.702406Z","iopub.status.idle":"2024-09-28T15:36:26.715122Z","shell.execute_reply":"2024-09-28T15:36:26.714229Z","shell.execute_reply.started":"2024-09-28T15:36:26.702750Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","def evaluate_clip_model(clip_model, dataloader, device, processor):\n","    clip_model.eval()  \n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)  \n","            def tensor_to_pil(image_tensor):\n","                image_tensor = image_tensor.permute(1, 2, 0).cpu().numpy() \n","                image_tensor = (image_tensor * 255).astype(np.uint8)  \n","                return Image.fromarray(image_tensor)\n","\n","            pil_images = [tensor_to_pil(image) for image in images]\n","\n","            image_inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n","\n","            text_inputs = processor(text=[f\"a photo of a {cifar10_labels[label.item()]}\" for label in labels],\n","                                    return_tensors=\"pt\", padding=True).to(device)\n","\n","            outputs = clip_model(**image_inputs, **text_inputs)\n","\n","            logits_per_image = outputs.logits_per_image \n","            predicted = logits_per_image.argmax(dim=1) \n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item() \n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:36:35.025110Z","iopub.status.busy":"2024-09-28T15:36:35.024401Z","iopub.status.idle":"2024-09-28T15:44:21.626035Z","shell.execute_reply":"2024-09-28T15:44:21.625050Z","shell.execute_reply.started":"2024-09-28T15:36:35.025069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Accuracy:\n","EfficientNet: 62.55%\n","ViT: 95.33%\n","CLIP-ViT: 6.71%\n"]}],"source":["original_acc_effnet = evaluate_model(efficient_net, cifar_loader, device)\n","original_acc_vit = evaluate_model(vit_model, cifar_loader, device)\n","original_acc_clip = evaluate_clip_model(clip_model, cifar_loader, device, clip_processor)\n","\n","print(\"Original Accuracy:\")\n","print(f\"EfficientNet: {original_acc_effnet:.2f}%\")\n","print(f\"ViT: {original_acc_vit:.2f}%\")\n","print(f\"CLIP-ViT: {original_acc_clip:.2f}%\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:44:49.343757Z","iopub.status.busy":"2024-09-28T15:44:49.342900Z","iopub.status.idle":"2024-09-28T15:44:49.357223Z","shell.execute_reply":"2024-09-28T15:44:49.356223Z","shell.execute_reply.started":"2024-09-28T15:44:49.343713Z"},"trusted":true},"outputs":[],"source":["\n","def add_local_noise(img, noise_level, device):\n","    noisy_img = img.clone().to(device)  \n","    noise = noise_level * torch.randn((3, 8, 8)).to(device)  \n","    noisy_img[:, :8, :8] = noisy_img[:, :8, :8] + noise \n","    return torch.clamp(noisy_img, -1, 1)  \n","\n","def evaluate_model_on_noisy_images(model, dataloader, noise_level, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device) \n","            noisy_images = torch.stack([add_local_noise(img, noise_level, device) for img in images])\n","            outputs = model(noisy_images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","def evaluate_clip_on_noisy_images(clip_model, dataloader, device, processor, noise_level=0.5):\n","    clip_model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            noisy_images = torch.stack([add_local_noise(img, noise_level, device) for img in images]).to(device)\n","            \n","            noisy_images_pil = [transforms.ToPILImage()(img).convert(\"RGB\") for img in noisy_images]\n","            \n","            inputs = processor(\n","                text=[f\"a photo of a {cifar10_test.classes[label]}\" for label in labels],\n","                images=noisy_images_pil,\n","                return_tensors=\"pt\",\n","                padding=True\n","            ).to(device)\n","            \n","            outputs = clip_model(**inputs)\n","            logits_per_image = outputs.logits_per_image\n","            \n","            predicted = logits_per_image.argmax(dim=1)\n","            total += labels.size(0)\n","            correct += (predicted.to(device) == labels.to(device)).sum().item()\n","    \n","    accuracy = 100 * correct / total\n","    return accuracy"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:44:56.188844Z","iopub.status.busy":"2024-09-28T15:44:56.187958Z","iopub.status.idle":"2024-09-28T15:52:47.847993Z","shell.execute_reply":"2024-09-28T15:52:47.847012Z","shell.execute_reply.started":"2024-09-28T15:44:56.188802Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Noisy Accuracy:\n","EfficientNet: 49.89%\n","ViT: 81.08%\n","CLIP-ViT: 7.26%\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n","\n","noisy_acc_effnet = evaluate_model_on_noisy_images(efficient_net, cifar_loader, noise_level=0.5, device=device)\n","noisy_acc_vit = evaluate_model_on_noisy_images(vit_model, cifar_loader, noise_level=0.5, device=device)\n","noisy_acc_clip = evaluate_clip_on_noisy_images(clip_model, cifar_loader, device, clip_processor, noise_level=0.5)\n","\n","\n","print(\"\\nNoisy Accuracy:\")\n","print(f\"EfficientNet: {noisy_acc_effnet:.2f}%\")\n","print(f\"ViT: {noisy_acc_vit:.2f}%\")\n","print(f\"CLIP-ViT: {noisy_acc_clip:.2f}%\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:52:55.383843Z","iopub.status.busy":"2024-09-28T15:52:55.383092Z","iopub.status.idle":"2024-09-28T15:52:55.394960Z","shell.execute_reply":"2024-09-28T15:52:55.393988Z","shell.execute_reply.started":"2024-09-28T15:52:55.383802Z"},"trusted":true},"outputs":[],"source":["def style_transfer_vgg(content_img, style_img):\n","    return content_img * 0.5 + style_img * 0.5\n","\n","def evaluate_model_on_styled_images(model, dataloader):\n","    correct = 0\n","    total = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            styled_images = torch.stack([style_transfer_vgg(img, img) for img in images]).to(device)\n","            outputs = model(styled_images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted.to(device) == labels.to(device)).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","def evaluate_clip_on_styled_images(clip_model, dataloader, device, processor):\n","    correct = 0\n","    total = 0\n","    clip_model.eval()\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            styled_images = torch.stack([style_transfer_vgg(img, img) for img in images])\n","            styled_images_pil = [transforms.ToPILImage()(img).convert(\"RGB\") for img in styled_images]\n","            inputs = processor(text=[f\"a photo of a {cifar10_test.classes[i]}\" for i in range(10)], images=styled_images_pil, return_tensors=\"pt\", padding=True).to(device)\n","            outputs = clip_model(**inputs)\n","            logits_per_image = outputs.logits_per_image\n","            predicted = logits_per_image.argmax(dim=1)\n","            total += labels.size(0)\n","            correct += (predicted.to(device) == labels.to(device)).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T15:52:59.540901Z","iopub.status.busy":"2024-09-28T15:52:59.539878Z","iopub.status.idle":"2024-09-28T16:00:59.315002Z","shell.execute_reply":"2024-09-28T16:00:59.313834Z","shell.execute_reply.started":"2024-09-28T15:52:59.540857Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Styled Accuracy:\n","EfficientNet: 62.55%\n","ViT: 95.33%\n","CLIP-ViT: 26.83%\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","styled_acc_effnet = evaluate_model_on_styled_images(efficient_net, cifar_loader)\n","styled_acc_vit = evaluate_model_on_styled_images(vit_model, cifar_loader)\n","styled_acc_clip = evaluate_clip_on_styled_images(clip_model, cifar_loader, device, clip_processor)\n","\n","\n","print(\"\\nStyled Accuracy:\")\n","print(f\"EfficientNet: {styled_acc_effnet:.2f}%\")\n","print(f\"ViT: {styled_acc_vit:.2f}%\")\n","print(f\"CLIP-ViT: {styled_acc_clip:.2f}%\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T16:01:45.849563Z","iopub.status.busy":"2024-09-28T16:01:45.848881Z","iopub.status.idle":"2024-09-28T16:01:45.863494Z","shell.execute_reply":"2024-09-28T16:01:45.862459Z","shell.execute_reply.started":"2024-09-28T16:01:45.849512Z"},"trusted":true},"outputs":[],"source":["def scramble_image(image, patch_size=8):\n","    c, h, w = image.shape\n","    patches = image.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n","    patches = patches.permute(1, 2, 0, 3, 4).reshape(-1, c, patch_size, patch_size)\n","\n","    np.random.shuffle(patches)\n","\n","    scrambled_img = patches.view(h // patch_size, w // patch_size, c, patch_size, patch_size).permute(2, 0, 3, 1, 4).reshape(c, h, w)\n","    return scrambled_img\n","\n","def evaluate_model_on_scrambled_images(model, dataloader):\n","    correct = 0\n","    total = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            scrambled_images = torch.stack([scramble_image(img) for img in images]).to(device)\n","            outputs = model(scrambled_images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted.to(device) == labels.to(device)).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","def evaluate_clip_on_scrambled_images(clip_model, dataloader, device, processor):\n","    correct = 0\n","    total = 0\n","    clip_model.eval()\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            scrambled_images = torch.stack([scramble_image(img) for img in images])\n","            scrambled_images_pil = [transforms.ToPILImage()(img).convert(\"RGB\") for img in scrambled_images]\n","            inputs = processor(text=[f\"a photo of a {cifar10_test.classes[i]}\" for i in range(10)], images=scrambled_images_pil, return_tensors=\"pt\", padding=True).to(device)\n","            outputs = clip_model(**inputs)\n","            logits_per_image = outputs.logits_per_image\n","            predicted = logits_per_image.argmax(dim=1)\n","            total += labels.size(0)\n","            correct += (predicted.to(device) == labels.to(device)).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T16:01:49.935442Z","iopub.status.busy":"2024-09-28T16:01:49.934761Z","iopub.status.idle":"2024-09-28T16:22:36.762716Z","shell.execute_reply":"2024-09-28T16:22:36.761744Z","shell.execute_reply.started":"2024-09-28T16:01:49.935401Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/3328192212.py:8: UserWarning: you are shuffling a 'Tensor' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n","  np.random.shuffle(patches)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Scrambled Accuracy:\n","EfficientNet: 9.25%\n","ViT: 17.25%\n","CLIP-ViT: 10.95%\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n","\n","scrambled_acc_effnet = evaluate_model_on_scrambled_images(efficient_net, cifar_loader)\n","scrambled_acc_vit = evaluate_model_on_scrambled_images(vit_model, cifar_loader)\n","scrambled_acc_clip = evaluate_clip_on_scrambled_images(clip_model, cifar_loader, device, clip_processor)\n","\n","\n","print(\"\\nScrambled Accuracy:\")\n","print(f\"EfficientNet: {scrambled_acc_effnet:.2f}%\")\n","print(f\"ViT: {scrambled_acc_vit:.2f}%\")\n","print(f\"CLIP-ViT: {scrambled_acc_clip:.2f}%\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"widgets":{"application/vnd.jupyter.widget-state+json":{"11c7a5e852664da5a5c055567ecb4e5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39aa549936e943d683baa01ae56cbb5b","IPY_MODEL_ec01489863174cc8bd538773071861fa","IPY_MODEL_8d6df37098ed4cacba31057f75ca2f27"],"layout":"IPY_MODEL_c76f3164643743398e7cc33df62d07ff"}},"1e4985e85f244dd9950445214a2d69e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233f1b5f44824ee2bb96b3206754d2a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb3ced2ed6014469b7493400b22664bd","max":598641023,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a014e3a6f80a4172a9625239b6c3688b","value":52428800}},"314765fc6d594e7cb915b9daee1e77c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33c6819a5b5746729431656651d5d389":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d18d321a83644b7948cc4cf8e05c427","placeholder":"​","style":"IPY_MODEL_4f9294c5440e4a1d9a5c6bb3b89ce1a9","value":"pytorch_model.bin:   9%"}},"350b50a272db45bd819d173a2cf23913":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_539efaf8f3e24b5bb798a236a3d7f0a8","placeholder":"​","style":"IPY_MODEL_801d74a454184088b230472ca66abde9","value":" 304M/599M [00:28&lt;00:50, 5.83MB/s]"}},"39aa549936e943d683baa01ae56cbb5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f37165e4f8949198fbd374d2d292b6c","placeholder":"​","style":"IPY_MODEL_fef9f6ffd0504be88d49a7dafab5256f","value":"config.json: 100%"}},"4f9294c5440e4a1d9a5c6bb3b89ce1a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"539efaf8f3e24b5bb798a236a3d7f0a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63046fc17f3c4f48bb33e6c9e6a06c35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f37165e4f8949198fbd374d2d292b6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fd4c5a0539e41edb8d9fd52e5f628e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a5eccf58634583b15dbf554c985f68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"747dc4a15ee74b458d4f7ba3877b8c0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d18d321a83644b7948cc4cf8e05c427":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"801d74a454184088b230472ca66abde9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8500995809c844a9a90f928fa6e7dcdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d6df37098ed4cacba31057f75ca2f27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b688d48c56e048528938c1823559ac0a","placeholder":"​","style":"IPY_MODEL_314765fc6d594e7cb915b9daee1e77c3","value":" 4.10k/4.10k [00:00&lt;00:00, 247kB/s]"}},"8e3756de2bf947c284e02d5227198d6e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a014e3a6f80a4172a9625239b6c3688b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b688d48c56e048528938c1823559ac0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb8da82efee401ab9dd12002f7bc99c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33c6819a5b5746729431656651d5d389","IPY_MODEL_233f1b5f44824ee2bb96b3206754d2a3","IPY_MODEL_e52bf05cac1a48fc8c864f7d6a141a93"],"layout":"IPY_MODEL_747dc4a15ee74b458d4f7ba3877b8c0b"}},"c76f3164643743398e7cc33df62d07ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb3ced2ed6014469b7493400b22664bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0734d0d730e44feaefe3738faf99046":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0815afbacef4d51918980d3ccb754e7","max":598641023,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8500995809c844a9a90f928fa6e7dcdc","value":304087040}},"d0815afbacef4d51918980d3ccb754e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4f742a1b724435d8170ecd873cfa623":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9189d84940f4b55be028bf2cf4e2851","IPY_MODEL_d0734d0d730e44feaefe3738faf99046","IPY_MODEL_350b50a272db45bd819d173a2cf23913"],"layout":"IPY_MODEL_8e3756de2bf947c284e02d5227198d6e"}},"e52bf05cac1a48fc8c864f7d6a141a93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fd4c5a0539e41edb8d9fd52e5f628e6","placeholder":"​","style":"IPY_MODEL_73a5eccf58634583b15dbf554c985f68","value":" 52.4M/599M [00:05&lt;00:53, 10.3MB/s]"}},"e82582a563864749b9780ca7b8a40a72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec01489863174cc8bd538773071861fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f866c2b81d8644d3ab914d2b85dd2347","max":4104,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63046fc17f3c4f48bb33e6c9e6a06c35","value":4104}},"f866c2b81d8644d3ab914d2b85dd2347":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9189d84940f4b55be028bf2cf4e2851":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e4985e85f244dd9950445214a2d69e8","placeholder":"​","style":"IPY_MODEL_e82582a563864749b9780ca7b8a40a72","value":"pytorch_model.bin:  51%"}},"fef9f6ffd0504be88d49a7dafab5256f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
